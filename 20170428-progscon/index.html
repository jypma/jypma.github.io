<!DOCTYPE html>
<html>
  <head>
    <title>Disruptive Development using Event Sourcing</title>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="animate.min.css">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .small { font-size: 75%; }
      .smallright { font-size: 90%; text-align: right; margin-top: 8em; }
      .floatright { float: right; }
      
      .remark-slide-content.hljs-default {
         background-size: 100%; 
      }
      .remark-slide-content {
        background: #F2F4F7;
         color: #060606;
         text-shadow: 1px 1px 2px rgba(0,0,0, 0.1);
      }
      .center img {
         background: rgba(255,255,255,0.3);
         border: 1px solid #EBEBE5;
         box-shadow: 1px 1px 2px rgba(0,0,0, 0.1);
      }
      .floatright img {
         background: rgba(255,255,255,0.3);
         border: 1px solid #EBEBE5;      
         box-shadow: 1px 1px 2px rgba(0,0,0, 0.1);
      }
      .good:before { 
         content: ' '; 
         display: block; 
      }
      .good {
         color: green;
      }
      .bad:before { 
         content: ' '; 
         display: block; 
      }
      .bad {
         color: red;
      }
      pre {
         text-shadow: none;
      }
      a {
        color: #7272B6;
      }
      .remark-slide-content > ul > li {
        padding-top: 5px;
      }
      h1 {
        margin-bottom: 19px;
      }
      @media print {
      .remark-slide-content {
        text-shadow: none;
      }      
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Disruptive Development 

using Reactive Event-Sourced Systems in Scala and Java


Jan Ypma

jyp@tradeshift.com

---

# Agenda

- A bit of Tradeshift history
- Typical sources of failures
- Concurrency models
- Event sourcing
- Implementation
- Real-world use cases

---

# Tradeshift in 2011

.center[![Tradeshift system](tsold.svg)]

- Electronic network to connect business
- Two major software components
  - _the frontend_
  - _the backend_
- &lt;10 developers

---

# Tradeshift in 2017

.center[![Tradeshift system](tsnew.svg)]

- 30 deployed components (and growing)
- 250 developers
- 250.000 LoC in _just the backend_

---

# Scaling of Tradeshift's systems

- Moore's law applies to AWS 
- Single point of _not quite failing often enough_


- 2016 directive:

    _All new components must be clustered_

- Yeah, what about the 30-ish existing ones?
  - New architecture is needed

---

# Scaling of Tradehift's development process

- 2011: _"We're a Java shop"_
--

- 2017: Not really, at least not anymore
  - Groovy and grails
  - Scala 
  - Python, Go, Ruby
  - Crazy javascript people
- But still, mostly Java  
  
- Empower teams to pick their own language and frameworks
  - but interact through well-described standards and APIs (REST)

---

# Typical sources of failures

.center[![Tradeshift flow](tsflow.svg)]

- We're down!
  - We overloaded the database
  - which caused the backend to respond slowly
  - which caused the frontend to respond slowly
  - which caused our users' web browsers to respond slowly
  - which caused our users to reload their page
  - `GOTO 10`


--

- What, we're not that special?
  - _Let it crash_ [2003, Amstrong]
  - Micro-services [2005, Rodgers]
  - The Reactive Manifesto [2013, BonÃ©r, Farley, Kuhn, Thompson]
  - Self-contained systems [2015, [scs-architecture.org](http://scs-architecture.org)]

---

# Self-contained systems

.floatright[![Async comm](async.svg)]

- No outgoing calls while handling an incoming request
  - (except to our own databases)
  - All inter-service communication must be **asynchronous**
  - This implies data replication


- No single points of failure
  - System must be clustered
  
  
- Design must trivially scale to 10x expected load

---

# The actor model

.floatright[![Actors](actors.svg)]

- **Actor** is an entity that responds to messages by
  - sending _messages_ to any other actors
  - creating child actors
  - performing side-effects

- A parent actor is the **Supervisor** of its child actors. On child actor failure, parent decides what to do:
  - Restart child (and grandchildren)
  - Stop child (and grandchildren)
  - Escalate
  
- **Akka** is a toolkit for writing actors on the JVM
  - _Java_: Actor is a normal class that extends `UntypedActor` or `AbstractActor`
  - _Scala_: Actor is a normal class that implements the `Actor` trait
  - **Message** is an immutable, serializable, Java class (`case class`)

---

# Reactive streams

- Imagine an HTTP request with a very long response, that reads 1.000.000 rows from a DB

*Using Threads*
--

- Just block while reading (waits for more data), and block while writing (TCP send buffer full)

*Using Actors*
- Let's have a "Request Actor" and a "Database Actor" 
  1. Request Actor messages Database Actor to requests the rows
  2. Database Actor sends back 1.000.000 messages as fast as it can?
--

- Needs some sort of `Ack` protocol between Request Actor and Database Actor, or batching

*Using Reactive streams*
- What if we could combine the advantages of asynchronous messaging and add in backpressure?
![Streams - terms](stream-terms.svg)

---

# Concurrency model summary

- **Threads and locks**
.good[&rarr;] Most of us know this
.good[&rarr;] Obvious way to stream data
.bad[&rarr;]  Limited scalability (max threads) and performance (context switch)
.bad[&rarr;]  Deadlocks
- **Futures / Promises**
.good[&rarr;] Easy to grasp
.good[&rarr;] Solves thread scalability
.bad[&rarr;]  Other performance limits (dispatcher queue instead of scheduler)
.bad[&rarr;]  Still need locks for shared data access, or limit to single thread
.bad[&rarr;]  Can't stream data
- **Actors**
.good[&rarr;] Easier to reach both high scalability and performance
.good[&rarr;] Handle shared data without locks
.bad[&rarr;]  Learning curve
.bad[&rarr;]  Can't stream data
- **Reactive streams**
.good[&rarr;] Builds on actors for scalability and performance
.good[&rarr;] Easier to grasp than full-on actors
.good[&rarr;] Streaming

---

# Event sourcing

.center[![Event sourcing](eventsource.svg)]

- System considers an append-only **Event journal** the only source of truth
- **Aggregate** is one unit of information to which (and only which) an event atomically applies
- Events  have a guaranteed order, but only within an aggregate


---

# Event sourcing

- Nice scalability properties
  - Each aggregate can process changes independently
- All information that spans >1 aggregate is materialized asynchronously into secundary data stores

.center[![Event sourcing - events](eventsource-events.svg)]


- Traditionally only applied inside a system 
  - Synchronous APIs ("_get customer history_")
  
- Why not expose event stream itself?
  - Eventual consistency
  - Latency implications
  - Security implications

---

class: center, middle

# Implementation

---

# Actor ping pong 

```
package akka;

trait Actor {
  def sender: ActorRef = ???            // this will return the sender of the current message "receive" is handling
  implicit def self: ActorRef = ???     // this is a reference to the current actor instance
  def context: ActorContext = ???       // interact with the actor system, create children, etc.
  
  def receive: PartialFunction[Any, Unit]    // handles received messages for this actor
}

trait ActorRef {
  def !(msg: Any)(implicit sender: ActorRef): Unit    // Sends a message to this actor
}
```

```
class PongActor extends Actor {
  override def receive = {
    case msg:String =>
      sender ! "pong"
  }
}
```

---

# Actor ping pong 

```
case object DoIt

class PingActor extends Actor {
  val pongActor: ActorRef = context.actorOf(Props[PongActor], "pongActor")
  var counter = 0

  override def receive = {
    case DoIt =>
      pongActor ! "ping"
    case msg:String =>
      counter += 1
      if (counter == 3) {
        context.system.shutdown()
      } else {
        sender ! "ping"
      }
  }
}
```

---

# Actor ping ping

```
object PingPong extends App {
    val system = ActorSystem()
    val pingActor = system.actorOf(Props[PingActor])
    pingActor ! DoIt
}
```

Output:
```
In PingActor - starting ping-pong
In PongActor - received message: ping
In PingActor - received message: pong
In PongActor - received message: ping
In PingActor - received message: pong
In PongActor - received message: ping
In PingActor - received message: pong
```

---

# Akka persistence

- Framework to do event sourcing using actors
- Persistence plugins for levelDB, cassandra, kafka, ...
- Each `PersistentActor` has a `String` identifier, under which events are stored

```
class ChatActor extends PersistentActor {
  override def persistenceId = "chat-1"
  
  var messages: Seq[String] = Seq.empty

  override def receiveCommand = {
    case "/list" => sender ! messages
    case msg: String => persist(msg) { evt =>
      messages :+= msg
      sender ! Done
    }
  }
  
  override def receiveRecover = {
    case msg: String => messages :+= msg
  }
}
```

---

# Akka remoting and clustering

- Transparently lets actors communicate between systems
- `ActorRef` can point to a remote actor
- Messages must be serializable (using configurable mechanisms)

```
    akka {
      actor {
        provider = "akka.remote.RemoteActorRefProvider"
      }
      remote {
        enabled-transports = ["akka.remote.netty.tcp"]
        netty.tcp {
          hostname = "127.0.0.1"
          port = 2552
        }
      }
      cluster {
        seed-nodes = [
          "akka.tcp://ClusterSystem@127.0.0.1:2551",
          "akka.tcp://ClusterSystem@127.0.0.1:2552"]
      }
    }
```

---

# Akka cluster sharding

- Dynamically distributes a group of actors across an akka cluster
- Cluster sharding needs to know where a particular message should go

```
case class ChatMessage(chatId: UUID, msg: String)

val extractEntityId: ShardRegion.ExtractEntityId = {
  case msg:ChatMessage => (msg.chatId.toString, msg)
}

val extractShardId: ShardRegion.ExtractShardId = {
  case msg:ChatMessage => (msg.chatId.getLeastSignificantBits % 100).toString
}
```

---

# Akka cluster sharding

- `ShardRegion` proxy sits between client and real (remote) persistent actor
- Persistent actor names will be their persistence id

```
class ChatActor extends PersistentActor {
  // ...
  override def persistenceId = self.path.name
}

val proxy: ActorRef = ClusterSharding(system).start(
            "conversations",
            Props[ChatActor],
            ClusterShardingSettings(system), 
            extractEntityId, extractShardId)

proxy ! ChatMessage(UUID.fromString("67c67d28-4719-4bf9-bfe6-3944ed961a60"), "hello!"))
```

---

# Akka streams

.center[![Streams - terms](stream-terms.svg)]

- `Graph` is a *blueprint* for a closed, finite network of *stages*, which communicate by streaming elements
- `GraphStage[S extends Shape]` is one processing stage within a graph, taking elements in through zero or more *Inlets*, and emitting through *Outlets*
- It's completely up to the stage when and how to respond to arriving elements
- All built-in graph stages embrace _backpressure_ and _bounded processing_

#### Mostly used graph stages
- `Source[T, M]` has one outlet of type `T`
- `Sink[T, M]` has one inlet of type `T` 
- `Flow[A, B, M]` has one inlet of type `A` and one outlet of type `B`
- `RunnableGraph[M]` has no inlets or outlets

---

# Hello, streams

```
val system = ActorSystem()
val materializer = ActorMaterializer(system)

val numbers: Source[Int, NotUsed] = Source.range(1, 100)
val format: Flow[Int, String, NotUsed] = Flow[Int].map(i => i.toString)
val print: Sink[String, Future[Done]] = Sink.foreach(i => System.out.println(i))

val done: Future[Done] = numbers.via(format).to(print).run(materializer)

// Output: 
// 1
// 2
// ...
```

---

# Event journal over HTTP

- We wanted easy consumption of the event log by other systems
- HTTP chunked encoding (1 chunk per event), without completion

```php
GET /events?since=1473689920000 HTTP/1.1

200 OK
Content-Type: application/protobuf
Transfer-Encoding: chunked

14
(14 bytes with the first protobuf event)
11
(11 bytes with the second protobuf event)
(TCP stream stalls here)
```

- Additional events can arrive in real time

- Ideal to use with akka streams

---

# Putting it all together

- 2015: Let's try it out first
- 2016: _Collaboration_ in production
  - Real-time text message exchange between employees
  - Text interface to automated travel agent
  
- 2017: Migrating _Network_
  - Global Tradeshift network graph

- Stuff that works well for us: [https://github.com/Tradeshift/ts-reaktive/](https://github.com/Tradeshift/ts-reaktive/)
  - `ts-reaktive-actors`: Persistent actor Java base classes with reasonable defaults, and HTTP API for event journal
  - `ts-reaktive-marshal`: Non-blocking streaming marshalling framework for akka streams
  - `ts-reaktive-replication`: Master-slave replication across data centers for persistent actors

---

# Use case: collaboration

.center[![Collaboration architecture](collaboration.svg)]

- _Content_ backend has API to add messages to conversations
  - Messages go into the event journal
  - Journal is queryable over HTTP
- _Presentation_ backend listens to event stream
  - Materializes into views that are UI dependent
  - Can combine other sources as well
- Browser talks to both Presentation and Content backends
- Web socket stream informs browser of incoming messages

---

# Use case: Business network routing

- Legacy backend system
  - &nbsp;&gt;500.000 company accounts with business identifiers
  - Currently in a relational database
  - Updated in-place
- Migration strategy
  1. Introduce event journal alongside existing data
  2. Write event sourced new identity microservice
  3. Start continuous import of events from the old system
  4. Swap over

.center[![Identity](identity.svg)]

---

# Wrap-up

- Scalable systems: check
- Scalable development: check

- We're not quite there yet
  - Akka, cassandra and the reaktive combo in active development
  - Attitude of _I've done Spring for 10+ years successfully, why would I learn this_
  - The proof is in more pudding

- Want to get involved?
  - Get:  [http://akka.io/](http://akka.io/)
  - Read: [http://doc.akka.io/docs/akka/current/java.html](http://doc.akka.io/docs/akka/current/java.html)
  - Chat: [https://gitter.im/akka/akka](https://gitter.im/akka/akka)
  - Hack: [https://github.com/Tradeshift/ts-reaktive/](https://github.com/Tradeshift/ts-reaktive/) and [https://github.com/jypma/ts-reaktive-examples/](https://github.com/jypma/ts-reaktive-examples/) 

---

class: center, middle

# Extra slides

---

# Introducing Akka Streams
- `Graph` is a *blueprint* for a closed, finite network of *stages*, which communicate by streaming elements
- `GraphStage<S extends Shape>` is one processing stage within a graph, taking elements in through zero or more *Inlets*, and emitting through *Outlets*
- It's completely up to the stage when and how to respond to arriving elements
- All built-in graph stages embrace _backpressure_ and _bounded processing_

#### Mostly used graph stages
- `Source<T, M>` has one outlet of type `T`
- `Sink<T, M>` has one inlet of type `T` 
- `Flow<A, B, M>` has one inlet of type `A` and one outlet of type `B`
- `RunnableGraph<M>` has no inlets or outlets

#### Reactive streams
- Akka is a reactive streams implementation (just like `RxJava` and others)
- You typically don't interact in terms of _publisher_ and _subscriber_ directly

---

# Hello, streams

```
final ActorSystem system = ActorSystem.create("QuickStart");
final Materializer materializer = ActorMaterializer.create(system);

final Source<Integer, NotUsed> numbers = Source.range(1, 100);

final Sink<Integer, CompletionStage<Done>> print = 
    Sink.foreach(i -> System.out.println(i));

final CompletionStage<Done> done = numbers.runWith(print, materializer);
  
// Output: 
// 1
// 2
// ...
```
---

# Stream materialization

- _Graph_ is only a blueprint: nothing runs until it's given to a _materializer_, typically `ActorMaterializer`
- All graph stages are generic in their materialized type `M`
- Graph can be materialized (`run`, `runWith`) more than once

```
class Source<T, M> {
  // A graph which materializes into the M2 of the sink (ignoring source's M)
  public RunnableGraph<M2> to(Sink<T,M2> sink);
  
  // Materializes, and returns the M of the sink (ignoring this source's M) 
  public <M2> M2 runWith(Sink<T, M2> sink, Materializer m) { ... }
  
  // A graph which materializes into the result of applying [combine] to 
  // this source's M and the sink's M2 
  public <M2, MR> RunnableGraph<MR> toMat(Sink<T,M2> sink, 
                                          Function2<M,M2,MR> combine);
}

class RunnableGraph<M> {
  public M run(Materializer m);
}
```


---

# Reusable pieces

- `Source`, `Sink` and `Flow` are all normal, immutable objects, so they're ideal to be
   constructed in reusable factory methods:

```
public Sink<String, CompletionStage<IOResult>> lineSink(String filename) {
  Sink<ByteString, CompletionStage<IOResult>> file = 
      FileIO.toPath(Paths.get(filename);

  // Let's start with some strings
  return Flow.of(String.class)    // Flow<String, String, NotUsed>
  
  // Convert them into bytes (UTF-8), adding a newline
  // We now have a Flow<String, ByteString, NotUsed>
    .map(s -> ByteString.fromString(s + "\n"))
    
  // Send them into a file, and we want the IOResult of the
  // FileIO sink as materialized value of our own sink
    .toMat(file), Keep.right());
}

numbers.runWith(lineSink("numbers.txt"), materializer);
```

---

# Time-based processing

```
final Source<Integer, NotUsed> numbers = Source.range(1, 100000000);

final Sink<Integer, CompletionStage<Done>> print = 
    Sink.foreach(i -> System.out.println(i));

final CompletionStage<Done> done = numbers
    .throttle(1, Duration.create(1, TimeUnit.SECONDS), 1, 
              ThrottleMode.shaping())
    .runWith(print, materializer);

```

- This does what you expect: print one message per second
- No `OutOfMemoryError`, akka buffers only as needed: _backpressure_
---

# Example Sources
- Materialize as `ActorRef`

```
Source<T, ActorRef> s = Source.actorRef(10, OverflowStrategy.fail());
```

- Materialize as reactive `Subscriber<T>`

```
Source<T, Subscriber<T>> s = Source.asSubscriber();
```

- Read from a reactive `Publisher<T> p`

```
Source<T, NotUsed> s = Source.fromPublisher(p);
```

- Emit the same element regularly

```
Source<T, Cancellable> s = Source.tick(duration, duration, element);
```
---

# Example Sinks
- Send to `ActorRef`

```
Sink<T, NotUsed> s = Sink.actorRef(target, "done");
```

- Materialize as reactive `Publisher<T>`

```
Sink<T, Publisher<T>> s = Sink.asPublisher(WITH_FANOUT);
```

- Materialize into a `java.util.List` of all elements

```
Sink<T, List<T>> s = Sink.seq();
```

---

# Example source and flow operators
- Send `Source<String, M> src` to an additional `Sink<String> sink`

```
Source<String, M> s = src.alsoTo(sink);
```

- Process batches of concatenated strings, but only if coming in too fast

```
Source<String, M> s = src.batchWeighted(1000, s -> s.length(), 
                                        s -> s, (s1,s2) -> s1 + s2);
```

- Process 1 seconds' worth of elements at a time, but at most 100

```
Source<List<String>, M> s = src.groupedWithin(100, Duration.create(1, SECONDS));
```

- Invoke a `CompletionStage` for each element, and resume with the results in order

```
CompletionStage<Integer> process(String s) { ... }
Source<String, M> s = src.mapAsync(this::process);
```

    </textarea>
    <script src="remark-latest.min.js"></script>
    <script>
      var slideshow = remark.create({
          ratio: '16:9',
          highlightLanguage: "scala",
          highlightLines: true
      });
    </script>
</html>
